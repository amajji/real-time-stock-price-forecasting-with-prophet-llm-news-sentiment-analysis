from transformers import AutoTokenizer, AutoModelForCausalLM



model_name = "meta/llama-1b-instruct-quantized"  
